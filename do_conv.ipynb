{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce258a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-24T05:10:43.799457Z",
     "start_time": "2022-05-24T05:10:43.739995Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections.abc as container_abcs\n",
    "from torch.nn import init\n",
    "from itertools import repeat\n",
    "from torch.nn import functional as F\n",
    "from torch._jit_internal import Optional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "int_classes = int\n",
    "\n",
    "class DOConv2d(Module):\n",
    "    \"\"\"\n",
    "       DOConv2d can be used as an alternative for torch.nn.Conv2d.\n",
    "       The interface is similar to that of Conv2d, with one exception:\n",
    "            1. D_mul: the depth multiplier for the over-parameterization.\n",
    "       Note that the groups parameter switchs between DO-Conv (groups=1),\n",
    "       DO-DConv (groups=in_channels), DO-GConv (otherwise).\n",
    "    \"\"\"\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size', 'D_mul']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, D_mul=None, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        super(DOConv2d, self).__init__()\n",
    "\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
    "\n",
    "        #################################### Initailization of D & W ###################################\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        self.D_mul = M * N if D_mul is None or M * N <= 1 else D_mul\n",
    "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, self.D_mul))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "        if M * N > 1:\n",
    "            self.D = Parameter(torch.Tensor(in_channels, M * N, self.D_mul))\n",
    "            init_zero = np.zeros([in_channels, M * N, self.D_mul], dtype=np.float32)\n",
    "            self.D.data = torch.from_numpy(init_zero)\n",
    "\n",
    "            eye = torch.reshape(torch.eye(M * N, dtype=torch.float32), (1, M * N, M * N))\n",
    "            D_diag = eye.repeat((in_channels, 1, self.D_mul // (M * N)))\n",
    "            if self.D_mul % (M * N) != 0:  # the cases when D_mul > M * N\n",
    "                zeros = torch.zeros([in_channels, M * N, self.D_mul % (M * N)])\n",
    "                self.D_diag = Parameter(torch.cat([D_diag, zeros], dim=2), requires_grad=False)\n",
    "            else:  # the case when D_mul = M * N\n",
    "                self.D_diag = Parameter(D_diag, requires_grad=False)\n",
    "        ##################################################################################################\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.W)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(DOConv2d, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        DoW_shape = (self.out_channels, self.in_channels // self.groups, M, N)\n",
    "        if M * N > 1:\n",
    "            ######################### Compute DoW #################\n",
    "            # (input_channels, D_mul, M * N)\n",
    "            D = self.D + self.D_diag\n",
    "            W = torch.reshape(self.W, (self.out_channels // self.groups, self.in_channels, self.D_mul))\n",
    "\n",
    "            # einsum outputs (out_channels // groups, in_channels, M * N),\n",
    "            # which is reshaped to\n",
    "            # (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(torch.einsum('ims,ois->oim', D, W), DoW_shape)\n",
    "            #######################################################\n",
    "        else:\n",
    "            # in this case D_mul == M * N\n",
    "            # reshape from\n",
    "            # (out_channels, in_channels // groups, D_mul)\n",
    "            # to\n",
    "            # (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(self.W, DoW_shape)\n",
    "        return self._conv_forward(input, DoW)\n",
    "\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, container_abcs.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "\n",
    "_pair = _ntuple(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
